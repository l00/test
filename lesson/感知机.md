## 引入分类问题





们先来把之前的线性回归推广到分类，我们说线性是在数据少的时候最有效的模型，因为它的参数足够小，整个数据集的信息都吸收在几个系数里。线性分类器的效果在连续特征的时候要远好于贝叶斯，因为对于任何一个维度，我们需要求解的仅仅是一个系数，而不需要求解条件概率一整条分布函数。　

 

这一类模型的特点是1，支撑模型参数选择的数据不需要很大  2，可以轻易的增加特征的维数，引入正则化进行过拟合的控制  3，测试速度快， 只需要调用模型参数，整个数据集的信息都已经吸收到参数里， 相比需要调用训练集的模型具有重大优势（或者说比KNN的实际参数小很多！）

 

我们可以用最简单的方法把它引入分类，得到线性分类器。我们强制做一个线性边界，把数据分割成不同的区域。 



## 感知机算法



### 感知机的生物学灵感



我们来看下第一个判别算法，生物进化流的感知机算法： 

在引言中介绍过感知机是由科学家[Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt)发明于1950至1960年代，他受到了来自[Warren McCulloch ](http://en.wikipedia.org/wiki/Warren_McCulloch)和[Walter Pitts](http://en.wikipedia.org/wiki/Walter_Pitts)的更早[工作](http://scholar.google.ca/scholar?cluster=4035975255085082870)的启发。感知机的灵感取自神经元，一个神经元， 由所谓的树突收集信息，轴突发射信息， 好比一个最简单的决策机器，树突收集的正是那些刚刚说的要权衡的要素， 而轴突就是根据输入是否超过一定阈值决定是放电还是不放电。整个生物神经元的树突和轴突都可以根据日常活动进行学习。 

现如今，我们通常使用其它种类的人工神经元模型——感知机是一个最古老的神经元类型， 直觉采用阈值函数作为神经元的决策（激活）函数。 未来还有各种其他类型的神经元是一种叫做sigmoid神经元（sigmoid neuron）的神经元模型。那么，感知机是怎么工作的呢？简而言之， 感知机的思维就是假设存在那么一条分类直线，然后这个神经元会测量错分点到这条线的距离之和， 然后用一个方法改变这个线的位置， 知道把一类点保存到线的一侧为止。 

感知机的输入是一系列连续的变量，x1,x2,…输出是一位单独的二进制：y 

### 如何把问题转化成数学 



我们好在几何上理解一下感知机是干什么的。它说的是求一个特征的线性组合， 记得线性代数告诉你的吗？

。比如刚刚举的那个去不去看电影的例子。 你要一个机器帮你做决策， 当然一开始它不了解你， 但是它可以先帮你做做决策， 然后根据你每次给他的反馈意见学到你的偏好， 这就是感知机训练的方法， 如果你给他肯定意见，他就不做改变， 你不满意， 他就变一变。 而怎么变， 我们就要搬出我们的高数了。 如果你来设计这样一个算法， 你要做什么呢?  首先，你写出目标函数，什么是合适的目标函数？ 就是让的训练的结果是你想要的。在这里，你要让你的预测少犯错，也就是错误最小。 什么是错误最小？ 就是分类错误的数据在w方向的投影值最小， 如果是0的话，就是没有错误。 而如果得不到0， 我也要它最小。

我们在用一个数学炫技，把二分类表示为正类和负类，取1和-1。 



### 感知机是如何训练的



误差的测量，转化为几何语言



公式推导：只有出错时候cost增加，分两种情况讨论， 正类为1， 预测为-1，距离xw+b 正类取-1 ，负类为-1， 预测为正1， 距离-wx-b，综合如下

​                              

示例：一个简单的二分类 

我们来理解一下这个函数，以及它的最值。首先， cost是相对w和b的函数。我们想象一下当w和b改变时候cost会如何变化。当yt不等于ytrue的时候数据出错， 从而使得cost增大。 等式右边的ytrue是已知的，yt却是由w，b决定，当他们改变， 总会由一定的点从分类正确变为错误， 或错误变为正确，所以这是一个阶跃变化的，不连续的函数。 我们需要寻找的是， 之前学过的求函数极大极小值的导数为0的点不能代数求出，单我们依然可以用梯度的思路进行优化，假设每个点周围局部可导，只不过这里的导数

我们依然取梯度运算， 求得cost对w和b的导数，然后求得梯度, 梯度告诉我们如何上山最快， 而它的反方向， 就是我们要更新的方西：

 

 

公式背后的直觉 

 【解释香蕉和苹果的例子】

用你的直觉，你能不能明白这个公式在说什么？ 仔细想想， 你的特征，可能由天下不下雨， 女朋友在家组成， 机器帮你决定你去不去公园，如果某一次，天下雨， 他让你去公园， 结果你揍了他， 机器就会减少这个要素的权重， 这样的话， 他下次让你去公园的概率就下降，直到你满意为止。那么为什么b要改变呢， 这是因为w变化了，为了保持平面上的直线位置不变，b也要相应调整。

就是这么简单。 

 

而这种更新和优化的方法，这是我们首次引入这个极重要的模型 ，又被称为sgd，为什么，因为我们不是基于整个数据集来更新参数，而是每次随机的抽取一个数据点来更新， 这样的好处在于可以减少每次计算的运算量，不必调用所有数据，方便实时更新，坏处是可能训练慢点 。 而且， 这种算法被证明在数据量很大的时候效率很高， 也是未来深度学习最常用的优化算法。



### 这个算法有什么问题





感知机有几个重大的弱点，使他其实只是一个历史阶段的保留，主要是：

１，线并不唯一确定

２，如果没有一条线能分开数据，算法不收敛

３，非黑即白的逻辑，无法给出概率，超过了那个界限， 你无论如何都是那一类了。  